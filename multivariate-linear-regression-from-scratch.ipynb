{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7311480a",
   "metadata": {
    "papermill": {
     "duration": 0.007858,
     "end_time": "2023-04-06T18:52:26.917807",
     "exception": false,
     "start_time": "2023-04-06T18:52:26.909949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction  <a id='introduction'></a>\n",
    "\n",
    "This notebook contains Python code for a multivariate linear regression task. Multivariate linear regression means linear regression with more than one variable. In addition to Python, we will also use Pandas, Numpy, and Matplotlib libraries. \n",
    "\n",
    "In my [Univariate Linear Regression from Scratch](https://www.kaggle.com/code/erkanhatipoglu/univariate-linear-regression-from-scratch) notebook, we have already seen basic Machine Learning concepts such as the hypothesis, cost function, and gradient descent. We will not explain those concepts in this notebook. Anyone interested may refer to my Kaggle notebook [Univariate Linear Regression from Scratch](https://www.kaggle.com/code/erkanhatipoglu/univariate-linear-regression-from-scratch) or my Medium blog post [Univariate Linear Regression from Scratch](https://medium.com/towards-artificial-intelligence/univariate-linear-regression-from-scratch-68065fe8eb09) published in [Towards AI](https://pub.towardsai.net/) for a more detailed explanation. Refer to my [Multivariate Linear Regression From Scratch](https://medium.com/towards-artificial-intelligence/multivariate-linear-regression-from-scratch-c6702e26cce0https://medium.com/towards-artificial-intelligence/multivariate-linear-regression-from-scratch-c6702e26cce0) blog post on Medium for more detailed discussions on concepts of this notebook. We will use a gradient descent solution and not discuss the 'normal equation.' The  'normal equation' is explained in the [Linear Regression Using the Normal Equation](https://www.kaggle.com/code/erkanhatipoglu/linear-regression-using-the-normal-equation) notebook.\n",
    "\n",
    "This notebook is greatly inspired by the famous Machine Learning course by [Andrew Ng](https://www.andrewng.org/). All the mistakes, if any, are made by me.\n",
    "\n",
    "Finally, thanks to [@Mohan S Acharya](https://www.kaggle.com/mohansacharya) for this dataset.\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "* [Introduction](#introduction)\n",
    "* [Vectorization](#vector)\n",
    "* [Helper functions](#functions)\n",
    "* [The Hypothesis](#hypothesis)\n",
    "* [The Cost Function](#cost)\n",
    "* [Gradient Descent](#gradient)\n",
    "* [Feature Scaling](#scaling)\n",
    "* [Loading Data](#getdata)\n",
    "* [Model Training](#training)\n",
    "* [Model Validation](#validation)\n",
    "* [Conclusion](#conclusion)\n",
    "* [References](#references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a06b892",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-06T18:52:26.934475Z",
     "iopub.status.busy": "2023-04-06T18:52:26.933757Z",
     "iopub.status.idle": "2023-04-06T18:52:28.319164Z",
     "shell.execute_reply": "2023-04-06T18:52:28.317945Z"
    },
    "papermill": {
     "duration": 1.396951,
     "end_time": "2023-04-06T18:52:28.322036",
     "exception": false,
     "start_time": "2023-04-06T18:52:26.925085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/graduate-admissions/Admission_Predict.csv\n",
      "/kaggle/input/graduate-admissions/Admission_Predict_Ver1.1.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "from sklearn.model_selection import train_test_split # Split data into train and validation sets\n",
    "import matplotlib.pyplot as plt # Data visualization\n",
    "from time import process_time # Calculate elapsed CPU time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660d8b9",
   "metadata": {
    "papermill": {
     "duration": 0.006617,
     "end_time": "2023-04-06T18:52:28.335761",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.329144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Vectorization   <a id='vector'></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a349b",
   "metadata": {
    "papermill": {
     "duration": 0.006629,
     "end_time": "2023-04-06T18:52:28.349255",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.342626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Instead of using loops in our Machine Learning functions, we can take advantage of matrices and vectors found in linear algebra libraries that are either built-in to a programming language or easily accessible. This concept is called vectorization.\n",
    "\n",
    "Those linear algebra libraries are generally well-written and highly optimized. So, by using them, our code will be more efficient and more straightforward. In this notebook, we will use the [NumPy](https://numpy.org/) and [Pandas](https://pandas.pydata.org/) libraries in Python for this purpose.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295f92d5",
   "metadata": {
    "papermill": {
     "duration": 0.006732,
     "end_time": "2023-04-06T18:52:28.362906",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.356174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper Functions   <a id='functions'></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ab1768",
   "metadata": {
    "papermill": {
     "duration": 0.008492,
     "end_time": "2023-04-06T18:52:28.379089",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.370597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> We will use some helper functions throughout the notebook. Collecting them in one place is a good idea, making the code more organized. First, we will define and explain those functions and then use them in our code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c441e7a",
   "metadata": {
    "papermill": {
     "duration": 0.008006,
     "end_time": "2023-04-06T18:52:28.394256",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.386250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The  Hypothesis   <a id='hypothesis'></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24707589",
   "metadata": {
    "papermill": {
     "duration": 0.00767,
     "end_time": "2023-04-06T18:52:28.409221",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.401551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The hypothesis for a linear regression task with multiple variables is of the form: \n",
    "\n",
    "\n",
    "$\\hat{y} = h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n$\n",
    "\n",
    "\n",
    "Where $\\hat{y} = h_\\theta(x)$ is the predicted target. The hypothesis will calculate the predicted values (or predicted targets) for a given set of inputs and $\\theta$ values ($\\theta_i$, where $i = 0,1,2,...,n$). \n",
    "\n",
    "\n",
    "Using vector notation, this equation can be rewritten as: \n",
    "\n",
    "\n",
    "$ h_\\theta(x) = \\begin{bmatrix}\\theta_{0} & \\theta_{1} & \\cdots & \\theta_{n}\n",
    "\\end{bmatrix} \\begin{bmatrix}x_{0}\\\\\n",
    "x_{1}\\\\\n",
    "\\vdots\\\\\n",
    "x_{n}\n",
    "\\end{bmatrix}= \\boldsymbol{\\theta}^T\\boldsymbol{x}$\n",
    "\n",
    "assuming $ \\boldsymbol{\\theta}$ and $\\boldsymbol{x}$ are column vectors and $\\boldsymbol{x_0 = 1}$.\n",
    "\n",
    "Each row in our dataset represents a sample such that:\n",
    "\n",
    "$ \\boldsymbol{X} = \\begin{bmatrix}x_{0}^{(1)} & x_{1}^{(1)} & \\cdots  & x_{n}^{(1)} \\\\\n",
    "x_{0}^{(2)} & x_{1}^{(2)} & \\cdots  & x_{n}^{(2)}\\\\\n",
    "\\vdots & \\vdots & \\vdots  & \\vdots\\\\\n",
    "x_{0}^{(m)} & x_{1}^{(m)} & \\cdots  & x_{n}^{(m)}\n",
    "\\end{bmatrix} and;$ \n",
    "\n",
    "$ \\boldsymbol{\\theta} = \\begin{bmatrix}\\theta_{0} \\\\\n",
    "\\theta_{1}\\\\\n",
    "\\vdots \\\\\n",
    "\\theta_{n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Where $ \\boldsymbol{X} $ is the training or test dataset, $ \\boldsymbol{\\theta} $ is the $ \\theta $ vector, m is the number of samples, and n is the number of features. As a result, we can calculate the hypothesis as a column vector of $ (m x 1) $ as follows: \n",
    "\n",
    "$ h_\\theta(x) = \\boldsymbol{X}\\boldsymbol{\\theta} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a67617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T18:52:28.424854Z",
     "iopub.status.busy": "2023-04-06T18:52:28.424478Z",
     "iopub.status.idle": "2023-04-06T18:52:28.430947Z",
     "shell.execute_reply": "2023-04-06T18:52:28.429215Z"
    },
    "papermill": {
     "duration": 0.017387,
     "end_time": "2023-04-06T18:52:28.433705",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.416318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The hypothesis\n",
    "def h(x, theta):\n",
    "    \"\"\"\n",
    "     Calculates the predicted values (or predicted targets) for a given set of input and theta vectors.\n",
    "    \n",
    "    :param x: inputs (feature values) - data frame of floats \n",
    "    :param theta: theta vector (weights) - Numpy array of floats\n",
    "    \n",
    "    :return: predicted targets - Numpy array of floats\n",
    "    \n",
    "    \"\"\"\n",
    "    # The hypothesis is a column vector of m x 1\n",
    "    return np.dot(x, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af21c9c",
   "metadata": {
    "papermill": {
     "duration": 0.006856,
     "end_time": "2023-04-06T18:52:28.447999",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.441143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The  Cost Function   <a id='cost'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1794853f",
   "metadata": {
    "papermill": {
     "duration": 0.006575,
     "end_time": "2023-04-06T18:52:28.461463",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.454888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will use the squared error cost function as in the univariate linear regression case. Therefore, the cost function is:\n",
    "\n",
    "$ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m} (h_{\\theta}(x^{(i)})-y^i)^2 $\n",
    "\n",
    "where $m$ is the number of examples, $h_\\theta()$ is the hypothesis function, $ x^i $ is the $i^{th}$ training example, and $y^i$ is the actual output of $i^{th}$ training example. And the vectorized form of the cost function is as follows:\n",
    "\n",
    "$ J(\\theta) = \\frac{1}{2m}( X\\theta - \\vec y)^T(X\\theta - \\vec y) $\n",
    "\n",
    "where $m$ is the number of examples, $X$ is the training or test dataset, $ \\boldsymbol{\\theta} $ is the $ \\theta $ vector, $\\vec y$ is the vector of corresponding $y$ values, and $()^T$ is the transpose of the matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d6535a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T18:52:28.476898Z",
     "iopub.status.busy": "2023-04-06T18:52:28.476490Z",
     "iopub.status.idle": "2023-04-06T18:52:28.483046Z",
     "shell.execute_reply": "2023-04-06T18:52:28.481989Z"
    },
    "papermill": {
     "duration": 0.017312,
     "end_time": "2023-04-06T18:52:28.485592",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.468280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The cost function\n",
    "\n",
    "def J(X,y,theta):\n",
    "    \"\"\"\n",
    "     Calculates the total error using squared error function.\n",
    "    \n",
    "    :param X: inputs (feature values) - data frame of floats\n",
    "    :param y: outputs (actual target values) - Numpy array of floats\n",
    "    :param theta: theta vector (weights) - Numpy array of floats\n",
    "    \n",
    "    :return: total error - float\n",
    "    \n",
    "    \"\"\"\n",
    "    # Calculate number of examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # Calculate the constant\n",
    "    c = 1/(2 * m)\n",
    "       \n",
    "    # Calculate the array of errors\n",
    "    temp_0 = h(X, theta) - y.reshape(-1)\n",
    "\n",
    "    # Calculate the transpose of array of errors\n",
    "    temp_1 = temp_0.transpose()\n",
    "\n",
    "    # Calculate the dot product \n",
    "    temp_2 = np.dot(temp_1, temp_0) \n",
    "\n",
    "    return  c * temp_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f06a2",
   "metadata": {
    "papermill": {
     "duration": 0.006793,
     "end_time": "2023-04-06T18:52:28.499760",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.492967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Gradient Descent   <a id='gradient'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c218de",
   "metadata": {
    "papermill": {
     "duration": 0.006927,
     "end_time": "2023-04-06T18:52:28.513812",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.506885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The gradient descent calculation is similar to what we have seen in the [univariate version](https://www.kaggle.com/code/erkanhatipoglu/univariate-linear-regression-from-scratch?kernelSessionId=106646889):\n",
    "\n",
    "$  \\text{repeat  until  convergence:  \\{} $\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)}).x_0^{(i)}$\n",
    "\n",
    "$\\theta_1 := \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)}).x_1^{(i)}$\n",
    "\n",
    "$\\theta_2 := \\theta_2 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)}).x_2^{(i)}$\n",
    "\n",
    "$\\cdots $\n",
    "\n",
    "$\\theta_n := \\theta_n - \\alpha\\frac{1}{m}\\sum_{i=1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)}).x_n^{(i)}$\n",
    "\n",
    "$  \\text{ \\}}$\n",
    "\n",
    "where\n",
    "\n",
    "$n$ is the number of features,\n",
    "\n",
    "$\\theta_j$ (for $j = 0,1,2,\\cdots,n$) is the corresponding weights for each feature, \n",
    "\n",
    "$\\alpha$ is the learning rate, \n",
    "\n",
    "$m$ is the number of examples, \n",
    "\n",
    "$h_{\\theta}(x^{(i)})$ is the result of the hypothesis function for the $i^{th}$ training example, \n",
    "\n",
    "$y^{(i)}$ is the actual value for the $i^{th}$ training example,\n",
    "\n",
    "and $x_k^{(i)}$ (for $k = 0,1,2,\\cdots,n$) is the value of the $k^{th}$ feature for the $i^{th}$ training example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f6d73",
   "metadata": {
    "papermill": {
     "duration": 0.006822,
     "end_time": "2023-04-06T18:52:28.527800",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.520978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Therefore, the vectorized form of the gradient descent is as follows: \n",
    "\n",
    "$ \\boldsymbol{\\theta} := \\boldsymbol{\\theta} - \\frac{\\alpha}{m} \\boldsymbol{X}^T (\\boldsymbol{X}\\boldsymbol{\\theta} - \\vec y) $\n",
    "\n",
    "where $ \\boldsymbol{\\theta} $ is the $ \\theta $ vector, $\\alpha$ is the learning rate, $m$ is the number of examples, $X$ is the training or test dataset, $()^T$ is the transpose of the matrix, and $\\vec y$ is the vector of corresponding $y$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b1d80e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T18:52:28.543796Z",
     "iopub.status.busy": "2023-04-06T18:52:28.543376Z",
     "iopub.status.idle": "2023-04-06T18:52:28.549875Z",
     "shell.execute_reply": "2023-04-06T18:52:28.548921Z"
    },
    "papermill": {
     "duration": 0.017477,
     "end_time": "2023-04-06T18:52:28.552447",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.534970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gradient descent function\n",
    "def gradient(X, y, theta, alpha):\n",
    "    \"\"\"\n",
    "     Calculates the gradient descent.\n",
    "    \n",
    "    :param X: inputs (feature values) - data frame of floats\n",
    "    :param y: outputs (actual target values) - Numpy array of floats\n",
    "    :param theta: theta vector (weights) - Numpy array of floats\n",
    "    :param alpha: learning rate\n",
    "    \n",
    "    :return: new theta - Numpy array of floats\n",
    "    \n",
    "    \"\"\"\n",
    "    # Calculate number of examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # Calculate the constant\n",
    "    c =  alpha / m\n",
    "        \n",
    "    # calculate the transpose of X\n",
    "    temp_0 = X.transpose()\n",
    "        \n",
    "    # Calculate the array of errors\n",
    "    temp_1 = h(X, theta) - y.reshape(-1) \n",
    "        \n",
    "    # Calculate the dot product \n",
    "    temp_2 = np.dot(temp_0, temp_1)\n",
    "        \n",
    "    return theta - (c * temp_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f84e1d",
   "metadata": {
    "papermill": {
     "duration": 0.007012,
     "end_time": "2023-04-06T18:52:28.567042",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.560030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Scaling  <a id='scaling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3568ca34",
   "metadata": {
    "papermill": {
     "duration": 0.006437,
     "end_time": "2023-04-06T18:52:28.580293",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.573856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Making the feature values approximately the same scale (i.e., a similar range of values) speeds up the gradient descent. Therefore in this notebook, we will scale some features in our dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4c3fa",
   "metadata": {
    "papermill": {
     "duration": 0.006513,
     "end_time": "2023-04-06T18:52:28.593670",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.587157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are various techniques for scaling; we will present three of them:\n",
    "\n",
    "**Standardization:** $\\frac{X-X.mean}{X.std}$\n",
    "\n",
    "**Mean Normalization** $\\frac{X-X.mean}{X.max - X.min}$\n",
    "\n",
    "**Min-Max Scaling:** $\\frac{X-X.min}{X.max - X.min}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98fff6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T18:52:28.609408Z",
     "iopub.status.busy": "2023-04-06T18:52:28.608749Z",
     "iopub.status.idle": "2023-04-06T18:52:28.616446Z",
     "shell.execute_reply": "2023-04-06T18:52:28.614800Z"
    },
    "papermill": {
     "duration": 0.018236,
     "end_time": "2023-04-06T18:52:28.618662",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.600426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalization and Standardization\n",
    "def normalize(X, columns):\n",
    "    \"\"\"\n",
    "     Applies feature scaling to the dataframe.\n",
    "    \n",
    "    :param X: unnormalized features - data frame of floats\n",
    "    :param columns: columns to be scaled - list of strings\n",
    "    \n",
    "    :return: normalized features - data frame of floats\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for column in columns:\n",
    "        \n",
    "        # Use this if you want Z-Score Normalization (or Standardization).\n",
    "        # Note that you must play with the learning rate\n",
    "        # and convergence threshold for better results.        \n",
    "        # X[column] = (X[column] - X[column].mean()) / X[column].std()\n",
    "        \n",
    "        # Use this if you want Mean Normalization.\n",
    "        # Note that you must play with the learning rate\n",
    "        # and convergence threshold for better results.        \n",
    "        # X[column] = (X[column] - X[column].mean()) / (X[column].max() - X[column].min()) or\n",
    "                \n",
    "        # Use this if you want Min-Max Scaling (or Min-Max Normalization).\n",
    "        # Note that you must play with the learning rate\n",
    "        # and convergence threshold for better results.  \n",
    "        # X[column] = (X[column] - X[column].min()) / (X[column].max() - X[column].min())\n",
    "        \n",
    "        # We will use Min-Max Scaling.\n",
    "        X[column] = (X[column] - X[column].min()) / (X[column].max() - X[column].min())\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d0f528",
   "metadata": {
    "papermill": {
     "duration": 0.006699,
     "end_time": "2023-04-06T18:52:28.632470",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.625771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading Data   <a id='getdata'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097b085",
   "metadata": {
    "papermill": {
     "duration": 0.007452,
     "end_time": "2023-04-06T18:52:28.647586",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.640134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First of all, we will load the CSV file in this part. Since there are two different versions of the dataset, we will load the one with more data. We will then create our training DataFrame and target vector. Next, we will normalize some columns of the training DataFrame using min-max scaling. Finally, we will split our data into training and validation DataFrames so that we can validate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ded341b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T18:52:28.664203Z",
     "iopub.status.busy": "2023-04-06T18:52:28.663850Z",
     "iopub.status.idle": "2023-04-06T18:52:28.711874Z",
     "shell.execute_reply": "2023-04-06T18:52:28.710381Z"
    },
    "papermill": {
     "duration": 0.05885,
     "end_time": "2023-04-06T18:52:28.713945",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.655095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.637821</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.368590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR       CGPA  Research\n",
       "107       0.96     0.892857                  4  3.5   4.5  0.852564         1\n",
       "336       0.58     0.642857                  3  3.0   2.5  0.637821         0\n",
       "71        0.92     0.714286                  5  5.0   5.0  0.948718         1\n",
       "474       0.36     0.464286                  4  3.0   2.5  0.368590         1\n",
       "6         0.62     0.607143                  3  3.0   4.0  0.448718         1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data. Note that there are two versions. We will use the one\n",
    "# with the most rows.\n",
    "\n",
    "train_data = pd.read_csv(\"/kaggle/input/graduate-admissions/Admission_Predict_Ver1.1.csv\")\n",
    "\n",
    "# Set X and y\n",
    "X = train_data.drop(['Chance of Admit ', 'Serial No.'], axis=1) # Chance of Admit is the target variable and Serial No. is the order. So we drop them.\n",
    "y = pd.DataFrame(data = train_data['Chance of Admit ']).to_numpy()\n",
    "\n",
    "# Select columns to be scaled\n",
    "columns = ['GRE Score', 'TOEFL Score', 'CGPA']\n",
    "\n",
    "# Min-max scaling\n",
    "X = normalize(X, columns)\n",
    "\n",
    "# Instead of finding probabilities, we want to calculate the percentages.\n",
    "y = y * 100\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state = 0)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1621259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T18:52:28.730180Z",
     "iopub.status.busy": "2023-04-06T18:52:28.729326Z",
     "iopub.status.idle": "2023-04-06T18:52:28.741912Z",
     "shell.execute_reply": "2023-04-06T18:52:28.740501Z"
    },
    "papermill": {
     "duration": 0.023362,
     "end_time": "2023-04-06T18:52:28.744492",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.721130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.426282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR       CGPA  Research\n",
       "90        0.56     0.500000                  2  4.0   4.0  0.358974         1\n",
       "254       0.62     0.785714                  4  4.0   5.0  0.743590         0\n",
       "283       0.62     0.678571                  3  2.5   3.0  0.673077         1\n",
       "445       0.76     0.857143                  5  4.5   5.0  0.730769         1\n",
       "461       0.22     0.357143                  3  2.5   2.0  0.426282         1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf123d4",
   "metadata": {
    "papermill": {
     "duration": 0.007674,
     "end_time": "2023-04-06T18:52:28.759929",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.752255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training   <a id='training'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e41c6",
   "metadata": {
    "papermill": {
     "duration": 0.007221,
     "end_time": "2023-04-06T18:52:28.774630",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.767409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As discussed in [The Hypothesis](#hypothesis) part, we need to define an $X_0$ feature equal to one for all training examples for a vectorized solution. So our first task is to insert the $X_0$ column into the training and validation data. Next, we need to initialize the $\\theta$ vector, which will be all zeros. As the last step, we will set the learning rate and threshold values. \n",
    "\n",
    "The threshold value will be used to check whether the gradient descent converges. To do that, we will subtract consecutive cost values in the while loop. If the difference is smaller than a certain threshold, we will conclude that the gradient descent converges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e68d8bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T18:52:28.791303Z",
     "iopub.status.busy": "2023-04-06T18:52:28.790889Z",
     "iopub.status.idle": "2023-04-06T18:52:28.828968Z",
     "shell.execute_reply": "2023-04-06T18:52:28.827307Z"
    },
    "papermill": {
     "duration": 0.049349,
     "end_time": "2023-04-06T18:52:28.831424",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.782075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial Cost: 2741.2200000000003\n",
      "\n",
      "Iteration: 1\n",
      "Calculated cost: 1803.536465562745\n",
      "cost difference: 937.6835344372553\n",
      "\n",
      "Iteration: 2\n",
      "Calculated cost: 1194.666514354635\n",
      "cost difference: 608.86995120811\n",
      "\n",
      "Iteration: 3\n",
      "Calculated cost: 799.3013538757673\n",
      "cost difference: 395.36516047886767\n",
      "\n",
      "Iteration: 4\n",
      "Calculated cost: 542.5688457291143\n",
      "cost difference: 256.732508146653\n",
      "\n",
      "Iteration: 5\n",
      "Calculated cost: 375.85312124304767\n",
      "cost difference: 166.71572448606662\n",
      "\n",
      "Iteration: 6\n",
      "Calculated cost: 267.586994808255\n",
      "cost difference: 108.26612643479268\n",
      "\n",
      "Iteration: 7\n",
      "Calculated cost: 197.2733070302654\n",
      "cost difference: 70.31368777798957\n",
      "\n",
      "Iteration: 8\n",
      "Calculated cost: 151.6028719171572\n",
      "cost difference: 45.670435113108226\n",
      "\n",
      "Iteration: 9\n",
      "Calculated cost: 121.9337879653226\n",
      "cost difference: 29.669083951834594\n",
      "\n",
      "Iteration: 10\n",
      "Calculated cost: 102.65470808144767\n",
      "cost difference: 19.279079883874928\n",
      "\n",
      "Iteration: 11\n",
      "Calculated cost: 90.12208037681452\n",
      "cost difference: 12.532627704633143\n",
      "\n",
      "Iteration: 12\n",
      "Calculated cost: 81.97007889780986\n",
      "cost difference: 8.152001479004667\n",
      "\n",
      "Iteration: 13\n",
      "Calculated cost: 76.66252884647793\n",
      "cost difference: 5.30755005133193\n",
      "\n",
      "Iteration: 14\n",
      "Calculated cost: 73.20196331315147\n",
      "cost difference: 3.4605655333264593\n",
      "\n",
      "Iteration: 15\n",
      "Calculated cost: 70.94070841511464\n",
      "cost difference: 2.2612548980368246\n",
      "\n",
      "Iteration: 16\n",
      "Calculated cost: 69.45821737222248\n",
      "cost difference: 1.482491042892164\n",
      "\n",
      "Iteration: 17\n",
      "Calculated cost: 68.4814212463933\n",
      "cost difference: 0.976796125829182\n",
      "\n",
      "Iteration: 18\n",
      "Calculated cost: 67.83301119903675\n",
      "cost difference: 0.6484100473565491\n",
      "\n",
      "Iteration: 19\n",
      "Calculated cost: 67.39785720128711\n",
      "cost difference: 0.43515399774963726\n",
      "\n",
      "Iteration: 20\n",
      "Calculated cost: 67.10120307952614\n",
      "cost difference: 0.29665412176096595\n",
      "\n",
      "Iteration: 21\n",
      "Calculated cost: 66.89450815389439\n",
      "cost difference: 0.2066949256317514\n",
      "\n",
      "Iteration: 22\n",
      "Calculated cost: 66.7462539514959\n",
      "cost difference: 0.14825420239849052\n",
      "\n",
      "Iteration: 23\n",
      "Calculated cost: 66.63597483471817\n",
      "cost difference: 0.11027911677773261\n",
      "\n",
      "Iteration: 24\n",
      "Calculated cost: 66.55038197744864\n",
      "cost difference: 0.08559285726953192\n",
      "\n",
      "Execution time: 24.621687999999864 milliseconds\n",
      "\n",
      "Calculated\u001b[1m θ\u001b[0m: [2.03692557 1.13926025 1.14289218 6.02902784 6.60181418 6.82324225\n",
      " 1.20521232 1.25048269]\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "\n",
    "# Calculate elapsed CPU time\n",
    "start = process_time()\n",
    "\n",
    "# Calculate the number of examles\n",
    "m_train = len(X_train)\n",
    "m_valid = len(X_valid)\n",
    "\n",
    "# Calculate the number of features\n",
    "# including X_0\n",
    "n = len(X_train.axes[1]) + 1\n",
    "\n",
    "# Create a list of ones\n",
    "ones_train = [1] * m_train\n",
    "ones_valid = [1] * m_valid\n",
    "\n",
    "# Insert ones to the fist column since\n",
    "# X_0 for all training examples should\n",
    "# be one.\n",
    "X_train.insert(0, \"X_0\", ones_train, True)\n",
    "X_valid.insert(0, \"X_0\", ones_valid, True)\n",
    "\n",
    "# Select zero vector for initial theta\n",
    "zero_list= [0] * n\n",
    "theta = np.asarray(zero_list)\n",
    "\n",
    "# set learning rate \n",
    "alpha = 0.005\n",
    "\n",
    "# Set convergence threshold\n",
    "threshold = 0.1\n",
    "\n",
    "# Initial cost value.\n",
    "# Will also be used in the first iteration\n",
    "# of the while loop. If the initial cost\n",
    "# is smaller then convergence threshold then\n",
    "# while loop will not be executed.\n",
    "cost_diff = J(X_train, y_train, theta)\n",
    "print(\"initial Cost: {}\".format(cost_diff))\n",
    "\n",
    "# We will count the number of iterations.\n",
    "my_iter = 0\n",
    "\n",
    "# Create a dictionary of cost values for debugging\n",
    "cost_dict = {} # will be used for storing the cost value of each iteration.\n",
    "\n",
    "# Add initial cost value to the dictionary\n",
    "my_key = \"I_\" + str(my_iter)\n",
    "cost_dict[my_key] = cost_diff\n",
    "\n",
    "# Start gradient descent\n",
    "while cost_diff >= threshold:\n",
    "    \n",
    "    # calculate initial cost value\n",
    "    initial_cost = J(X_train, y_train, theta)\n",
    "    \n",
    "    # calculate and assign the new theta values\n",
    "    theta = gradient(X_train, y_train, theta, alpha)\n",
    "    \n",
    "    # calculate the consecutive cost value\n",
    "    new_cost = J(X_train, y_train, theta)\n",
    "        \n",
    "    # calculate the difference between the consecutive\n",
    "    # cost values\n",
    "    cost_diff = initial_cost - new_cost\n",
    "    \n",
    "    # Update the dictionary\n",
    "    my_key = \"I_\" + str(my_iter)\n",
    "    cost_dict[my_key] = new_cost\n",
    "    \n",
    "    my_iter += 1\n",
    "    \n",
    "    print()\n",
    "    print(\"Iteration: {}\".format(my_iter))\n",
    "    print(\"Calculated cost: {}\".format(new_cost))\n",
    "    print(\"cost difference: {}\".format(cost_diff))\n",
    "\n",
    "# Calculate elapsed CPU time\n",
    "end = process_time()\n",
    "execution_time = (end - start)*1000\n",
    "\n",
    "# display theta and cpu execution time of training\n",
    "print(\"\\nExecution time: {} milliseconds\".format(execution_time))\n",
    "print(\"\\nCalculated\\033[1m θ\\033[0m: {}\".format(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da652d69",
   "metadata": {
    "papermill": {
     "duration": 0.008081,
     "end_time": "2023-04-06T18:52:28.847268",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.839187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For debugging purposes, we can draw learning curves. A learning curve created with the training process data is called the training learning curve. The x-axis of the learning curve is the number of iterations, and the y-axis is the calculated cost value in each iteration. We must be sure that the cost value decreases in each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378bc067",
   "metadata": {
    "papermill": {
     "duration": 0.007672,
     "end_time": "2023-04-06T18:52:28.862983",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.855311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> An iteration count larger than 50 may cause the x-axis of the figure to be unreadable. Any reader reproducing the code may want to change the dictionary key values used to draw the x-axis, especially for low learning rate values. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09f263ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T18:52:28.881817Z",
     "iopub.status.busy": "2023-04-06T18:52:28.881390Z",
     "iopub.status.idle": "2023-04-06T18:52:29.180770Z",
     "shell.execute_reply": "2023-04-06T18:52:29.179051Z"
    },
    "papermill": {
     "duration": 0.312194,
     "end_time": "2023-04-06T18:52:29.183986",
     "exception": false,
     "start_time": "2023-04-06T18:52:28.871792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7054fd6fbcd0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Training Learning Curve\n",
    "Y1=[x for x in cost_dict.values()]\n",
    "\n",
    "X1=[x for x in cost_dict.keys()]\n",
    "\n",
    "fig = plt.figure(figsize=[20, 10])\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('J(θ)')\n",
    "plt.title('The Training Learning Curve')\n",
    "plt.plot(X1,Y1, color='tab:blue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b93c2",
   "metadata": {
    "papermill": {
     "duration": 0.008108,
     "end_time": "2023-04-06T18:52:29.201100",
     "exception": false,
     "start_time": "2023-04-06T18:52:29.192992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Validation   <a id='validation'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e248e0e",
   "metadata": {
    "papermill": {
     "duration": 0.008219,
     "end_time": "2023-04-06T18:52:29.217791",
     "exception": false,
     "start_time": "2023-04-06T18:52:29.209572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As described in the previous section, we can use learning curves to debug our model. Our learning curve seems all right, and its shape is just as expected. But we still need to validate our model. To do that, we have already split our data into training and validation datasets before training starts since, for validation, we must use data that has never been used in the training process.\n",
    "\n",
    "We will use the following  $\\theta$ vector, calculated in the [Model Training](#training) section:\n",
    "\n",
    "$\\boldsymbol{\\theta}: [\\theta_0 = 2.03692557 \\text{, }  \\theta_1 = 1.13926025\\text{, } \\theta_2 = 1.14289218\\text{, } \\theta_3 = 6.02902784\\text{, } \\theta_4 = 6.60181418\\text{, } \\theta_5 = 6.82324225\\text{, } \\theta_6 = 1.20521232 \\text{, } \\theta_7 = 1.25048269]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cbae8e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T18:52:29.236929Z",
     "iopub.status.busy": "2023-04-06T18:52:29.236150Z",
     "iopub.status.idle": "2023-04-06T18:52:29.242900Z",
     "shell.execute_reply": "2023-04-06T18:52:29.241287Z"
    },
    "papermill": {
     "duration": 0.019018,
     "end_time": "2023-04-06T18:52:29.245159",
     "exception": false,
     "start_time": "2023-04-06T18:52:29.226141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \u001b[1m θ\u001b[0m: [2.03692557 1.13926025 1.14289218 6.02902784 6.60181418 6.82324225\n",
      " 1.20521232 1.25048269]\n",
      "\n",
      "Cost of test data: 76.48149798139413\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n \\033[1m θ\\033[0m: {}\\n\".format(theta))\n",
    "\n",
    "# calculate the cost value for the test set\n",
    "cost_test = J(X_valid, y_valid, theta)\n",
    "print(\"Cost of test data: {}\".format(cost_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe9f9f",
   "metadata": {
    "papermill": {
     "duration": 0.008084,
     "end_time": "2023-04-06T18:52:29.261771",
     "exception": false,
     "start_time": "2023-04-06T18:52:29.253687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The cost of the validation dataset seems slightly bigger (worse) than the training dataset, which is expected. We can also see the validation results below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5183e9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T18:52:29.281202Z",
     "iopub.status.busy": "2023-04-06T18:52:29.280632Z",
     "iopub.status.idle": "2023-04-06T18:52:29.293563Z",
     "shell.execute_reply": "2023-04-06T18:52:29.291994Z"
    },
    "papermill": {
     "duration": 0.026932,
     "end_time": "2023-04-06T18:52:29.297344",
     "exception": false,
     "start_time": "2023-04-06T18:52:29.270412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual CoA</th>\n",
       "      <th>Predicted CoA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>64.0</td>\n",
       "      <td>70.687762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>85.0</td>\n",
       "      <td>89.177016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>80.0</td>\n",
       "      <td>60.641830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>91.0</td>\n",
       "      <td>99.983114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>68.0</td>\n",
       "      <td>52.698085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual CoA  Predicted CoA\n",
       "90         64.0      70.687762\n",
       "254        85.0      89.177016\n",
       "283        80.0      60.641830\n",
       "445        91.0      99.983114\n",
       "461        68.0      52.698085"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(index=X_valid.index)\n",
    "result['Actual CoA'] = y_valid\n",
    "result['Predicted CoA'] = h(X_valid, theta)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791ffd1",
   "metadata": {
    "papermill": {
     "duration": 0.008831,
     "end_time": "2023-04-06T18:52:29.315723",
     "exception": false,
     "start_time": "2023-04-06T18:52:29.306892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion   <a id='conclusion'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7460b2c",
   "metadata": {
    "papermill": {
     "duration": 0.009388,
     "end_time": "2023-04-06T18:52:29.334094",
     "exception": false,
     "start_time": "2023-04-06T18:52:29.324706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this notebook, we have written the hypothesis, cost function, and gradient descent algorithms in Python with a vectorization method for a multivariate Linear Regression task. We have also normalized a portion of our data, trained a Linear Regression model, and validated it by splitting our data.\n",
    "\n",
    "We set the learning rate high and the convergence threshold low for demonstration purposes, resulting in a learning cost of approximately 66 and a validation cost of roughly 76. This cost is somewhat high, which can be easily seen by checking the predicted and actual ***Chance of Admit*** values. By playing with the learning rate and threshold values, we can get a training cost value smaller than 20, resulting in better predictions. \n",
    "\n",
    "Please feel free to correct me if I've made mistakes, and also, if you need help implementing the code, just let me know via the comments. I will try to answer as soon as possible.\n",
    "\n",
    "Thank you for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44764984",
   "metadata": {
    "papermill": {
     "duration": 0.009246,
     "end_time": "2023-04-06T18:52:29.352154",
     "exception": false,
     "start_time": "2023-04-06T18:52:29.342908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References   <a id='references'></a>\n",
    "* [Machine Learning Specialization - Deeplearning.AI](https://www.deeplearning.ai/program/machine-learning-specialization/)\n",
    "* [Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng)\n",
    "* [@Mohan S Acharya](https://www.kaggle.com/mohansacharya)\n",
    "* [10-simple-hacks-to-speed-up-your-data-analysis - Parul Pandey](https://www.kaggle.com/parulpandey/10-simple-hacks-to-speed-up-your-data-analysis)\n",
    "* [Univariate Linear Regression From Scratch - Kaggle](https://www.kaggle.com/code/erkanhatipoglu/univariate-linear-regression-from-scratch)\n",
    "* [Univariate Linear Regression From Scratch - Towards AI](https://pub.towardsai.net/univariate-linear-regression-from-scratch-68065fe8eb09)\n",
    "* [Multivariate Linear Regression From Scratch - Towards AI](https://medium.com/towards-artificial-intelligence/multivariate-linear-regression-from-scratch-c6702e26cce0https://medium.com/towards-artificial-intelligence/multivariate-linear-regression-from-scratch-c6702e26cce0)\n",
    "* [Towards AI](https://pub.towardsai.net/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.906683,
   "end_time": "2023-04-06T18:52:30.084566",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-06T18:52:17.177883",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
